{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "882658ca",
   "metadata": {},
   "source": [
    "# Traning RNN on Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b502ee0b",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c39ccc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programme\\Anaconda\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Programme\\Anaconda\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "ee780ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import pickle\n",
    "import Simulation.KNNSim as sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b05cb4",
   "metadata": {},
   "source": [
    "### Load data and helper tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4db6706d",
   "metadata": {},
   "outputs": [],
   "source": [
    "startIndexes = pickle.load(open('Simulation/E0F1_index_transmision.pkl', 'rb'))\n",
    "usablePVindextable = pickle.load(open('Simulation/E0F1_index_I1.pkl', 'rb'))\n",
    "pvNames = pickle.load(open('Simulation/E0F1Names.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85465584",
   "metadata": {},
   "source": [
    "### Define the shape of the input..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "93d85357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim =  len(usablePVindextable)*2 + 1 # permissionbools for PVs (dependand on subsection),current PV values , current current value\n",
    "output_dim = len(usablePVindextable)\n",
    "input_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950b8d9f",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "dbcd8423",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.SimpleRNN(32, return_sequences=True, stateful=True, batch_input_shape=(1,None,input_dim)),\n",
    "    tf.keras.layers.SimpleRNN(32, return_sequences=True, stateful=True),\n",
    "    tf.keras.layers.SimpleRNN(16, return_sequences=True, stateful=True),\n",
    "    tf.keras.layers.SimpleRNN(16, return_sequences=False, stateful=True),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(output_dim, activation='linear') # Adjust based on your output requirements\n",
    "])\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.build()\n",
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "313ad187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn (SimpleRNN)      (1, None, 32)             1856      \n",
      "                                                                 \n",
      " simple_rnn_1 (SimpleRNN)    (1, None, 32)             2080      \n",
      "                                                                 \n",
      " simple_rnn_2 (SimpleRNN)    (1, None, 16)             784       \n",
      "                                                                 \n",
      " simple_rnn_3 (SimpleRNN)    (1, 16)                   528       \n",
      "                                                                 \n",
      " dense_59 (Dense)            (1, 32)                   544       \n",
      "                                                                 \n",
      " dense_60 (Dense)            (1, 16)                   528       \n",
      "                                                                 \n",
      " dense_61 (Dense)            (1, 16)                   272       \n",
      "                                                                 \n",
      " dense_62 (Dense)            (1, 12)                   204       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6796 (26.55 KB)\n",
      "Trainable params: 6796 (26.55 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875edc1b",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "78cf5e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateRandomBoolArr(length, numOfOnes):\n",
    "    result = np.zeros(length, dtype=int)\n",
    "    indices = np.random.choice(length, numOfOnes, replace=False)\n",
    "    result[indices] = 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "8da8f548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelOutToPosition(modelOut, allowedPVs, oldPosition):\n",
    "    for i, change in enumerate(modelOut):\n",
    "        if allowedPVs[i]:\n",
    "            oldPosition[usablePVindextable[i]] += change\n",
    "        modelOut[i] = oldPosition[usablePVindextable[i]]\n",
    "    return [oldPosition, modelOut]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "859d0a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def biasedRandomNumber(max_value):\n",
    "    prob = np.arange(1, max_value + 1)\n",
    "    # Normalize probabilities to sum to 1\n",
    "    prob = prob / np.sum(prob)\n",
    "    result = max_value + 1 - np.random.choice(np.arange(1, max_value + 1), p=prob)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "e029161a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.529\n"
     ]
    }
   ],
   "source": [
    "#40104 is the index for the global optimum\n",
    "optimum = list(sim.states[40104][0])\n",
    "baseOptimum = []\n",
    "\n",
    "for i in usablePVindextable:\n",
    "    baseOptimum.append(optimum[i])\n",
    "\n",
    "var_I1 = [0.4656249999999993,0.17000000000000035,0.6524999999999999,0.14566250000000042,0.34001250000000066,0.19999999999999996,0.14499999999999957,0.13628749999999912,2.239959716796875,1.40057373046875,2.3402099609375,1.9005126953125]\n",
    "\n",
    "def get_StartingPossition():\n",
    "    sectionPos = []\n",
    "    for i, variance in enumerate(var_I1):\n",
    "        newValue = baseOptimum[i] + (random.random()-0.5) * variance\n",
    "        optimum[usablePVindextable[i]] = newValue\n",
    "        sectionPos.append(newValue)\n",
    "    return [np.array(optimum), np.array(sectionPos)]\n",
    "\n",
    "count = 0\n",
    "for i in range(1000):\n",
    "    get_StartingPossition()\n",
    "    if sim.get(optimum) < 0:\n",
    "        count += 1\n",
    "print(count/1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7c4ba0",
   "metadata": {},
   "source": [
    "## Trainig Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "56569915",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Traning...\n",
      "Epoch 1/4000, AVG Loss:  6.085, AVG improvment: -15.062, Acc:  32.0%\n",
      "------------------------ New best Dec with -4.820 ------------------------\n",
      "Modelsnapshot saved as:ModelRNNE0_L 6.09_I-15.06_P 0.32\n",
      "Epoch 2/4000, AVG Loss:  8.556, AVG improvment: -18.853, Acc:  33.0%\n",
      "------------------------ New best Dec with -6.221 ------------------------\n",
      "Epoch 3/4000, AVG Loss:  9.223, AVG improvment: -11.643, Acc:  29.0%\n",
      "Epoch 4/4000, AVG Loss:  10.558, AVG improvment: -14.238, Acc:  25.0%\n",
      "Epoch 5/4000, AVG Loss:  6.710, AVG improvment: -19.881, Acc:  35.0%\n",
      "------------------------ New best Dec with -6.958 ------------------------\n",
      "Epoch 6/4000, AVG Loss:  4.264, AVG improvment: -22.362, Acc:  42.0%\n",
      "------------------------ New best Dec with -9.392 ------------------------\n",
      "Epoch 7/4000, AVG Loss:  11.310, AVG improvment: -13.590, Acc:  28.0%\n",
      "Epoch 8/4000, AVG Loss:  10.756, AVG improvment: -12.715, Acc:  21.0%\n",
      "Epoch 9/4000, AVG Loss:  9.413, AVG improvment: -11.481, Acc:  22.0%\n",
      "Epoch 10/4000, AVG Loss:  8.859, AVG improvment: -14.272, Acc:  28.0%\n",
      "Epoch 11/4000, AVG Loss:  9.156, AVG improvment: -9.095, Acc:  28.0%\n",
      "Epoch 12/4000, AVG Loss:  7.148, AVG improvment: -16.283, Acc:  30.0%\n",
      "Epoch 13/4000, AVG Loss:  5.189, AVG improvment: -24.812, Acc:  37.0%\n",
      "Epoch 14/4000, AVG Loss:  8.867, AVG improvment: -13.136, Acc:  29.0%\n",
      "Epoch 15/4000, AVG Loss:  7.025, AVG improvment: -15.915, Acc:  28.0%\n",
      "Epoch 16/4000, AVG Loss:  8.262, AVG improvment: -12.812, Acc:  27.0%\n",
      "Epoch 17/4000, AVG Loss:  9.090, AVG improvment: -12.393, Acc:  30.0%\n",
      "Epoch 18/4000, AVG Loss:  9.104, AVG improvment: -9.049, Acc:  24.0%\n",
      "Epoch 19/4000, AVG Loss:  6.446, AVG improvment: -16.211, Acc:  32.0%\n",
      "Epoch 20/4000, AVG Loss:  7.325, AVG improvment: -13.879, Acc:  30.0%\n",
      "Epoch 21/4000, AVG Loss:  8.665, AVG improvment: -14.590, Acc:  30.0%\n",
      "Epoch 22/4000, AVG Loss:  9.260, AVG improvment: -8.841, Acc:  22.0%\n",
      "Epoch 23/4000, AVG Loss:  7.011, AVG improvment: -20.064, Acc:  38.0%\n",
      "Epoch 24/4000, AVG Loss:  8.362, AVG improvment: -15.689, Acc:  33.0%\n",
      "Epoch 25/4000, AVG Loss:  7.298, AVG improvment: -12.959, Acc:  29.0%\n",
      "Epoch 26/4000, AVG Loss:  3.933, AVG improvment: -15.298, Acc:  31.0%\n",
      "Epoch 27/4000, AVG Loss:  6.580, AVG improvment: -14.418, Acc:  27.0%\n",
      "Epoch 28/4000, AVG Loss:  7.619, AVG improvment: -17.450, Acc:  29.0%\n",
      "Epoch 29/4000, AVG Loss:  7.273, AVG improvment: -18.918, Acc:  37.0%\n",
      "Epoch 30/4000, AVG Loss:  9.985, AVG improvment: -12.208, Acc:  37.0%\n",
      "Epoch 31/4000, AVG Loss:  7.805, AVG improvment: -17.823, Acc:  33.0%\n",
      "Epoch 32/4000, AVG Loss:  10.223, AVG improvment: -12.101, Acc:  23.0%\n",
      "Epoch 33/4000, AVG Loss:  4.917, AVG improvment: -14.260, Acc:  33.0%\n",
      "Epoch 34/4000, AVG Loss:  7.050, AVG improvment: -11.836, Acc:  17.0%\n",
      "Epoch 35/4000, AVG Loss:  7.119, AVG improvment: -12.516, Acc:  21.0%\n",
      "Epoch 36/4000, AVG Loss:  8.025, AVG improvment: -9.544, Acc:  21.0%\n",
      "Epoch 37/4000, AVG Loss:  7.870, AVG improvment: -8.822, Acc:  23.0%\n",
      "Epoch 38/4000, AVG Loss:  7.674, AVG improvment: -6.712, Acc:  20.0%\n",
      "Epoch 39/4000, AVG Loss:  9.144, AVG improvment: -13.724, Acc:  25.0%\n",
      "Epoch 40/4000, AVG Loss:  8.496, AVG improvment: -8.747, Acc:  20.0%\n",
      "Epoch 41/4000, AVG Loss:  7.832, AVG improvment: -14.811, Acc:  26.0%\n",
      "Epoch 42/4000, AVG Loss:  8.951, AVG improvment: -10.437, Acc:  24.0%\n",
      "Epoch 43/4000, AVG Loss:  11.024, AVG improvment: -9.075, Acc:  16.0%\n",
      "Epoch 44/4000, AVG Loss:  7.030, AVG improvment: -11.952, Acc:  24.0%\n",
      "Epoch 45/4000, AVG Loss:  9.035, AVG improvment: -11.236, Acc:  19.0%\n",
      "Epoch 46/4000, AVG Loss:  11.337, AVG improvment: -9.772, Acc:  17.0%\n",
      "Epoch 47/4000, AVG Loss:  7.399, AVG improvment: -13.482, Acc:  28.0%\n",
      "Epoch 48/4000, AVG Loss:  8.517, AVG improvment: -12.868, Acc:  30.0%\n",
      "Epoch 49/4000, AVG Loss:  10.464, AVG improvment: -10.742, Acc:  29.0%\n",
      "Epoch 50/4000, AVG Loss:  8.419, AVG improvment: -9.260, Acc:  20.0%\n",
      "Epoch 51/4000, AVG Loss:  7.391, AVG improvment: -13.982, Acc:  22.0%\n",
      "Epoch 52/4000, AVG Loss:  11.719, AVG improvment: -9.591, Acc:  22.0%\n",
      "Epoch 53/4000, AVG Loss:  9.616, AVG improvment: -15.081, Acc:  27.0%\n",
      "Epoch 54/4000, AVG Loss:  9.320, AVG improvment: -12.598, Acc:  28.0%\n",
      "Epoch 55/4000, AVG Loss:  6.996, AVG improvment: -9.806, Acc:  26.0%\n",
      "Epoch 56/4000, AVG Loss:  7.925, AVG improvment: -11.897, Acc:  23.0%\n",
      "Epoch 57/4000, AVG Loss:  8.390, AVG improvment: -13.829, Acc:  25.0%\n",
      "Epoch 58/4000, AVG Loss:  5.197, AVG improvment: -21.651, Acc:  30.0%\n",
      "Epoch 59/4000, AVG Loss:  8.662, AVG improvment: -10.673, Acc:  24.0%\n",
      "Epoch 60/4000, AVG Loss:  9.587, AVG improvment: -9.067, Acc:  22.0%\n",
      "Epoch 61/4000, AVG Loss:  8.478, AVG improvment: -9.230, Acc:  16.0%\n",
      "Epoch 62/4000, AVG Loss:  10.412, AVG improvment: -5.837, Acc:  17.0%\n",
      "Epoch 63/4000, AVG Loss:  7.327, AVG improvment: -6.280, Acc:  16.0%\n",
      "Epoch 64/4000, AVG Loss:  7.896, AVG improvment: -9.778, Acc:  22.0%\n",
      "Epoch 65/4000, AVG Loss:  8.720, AVG improvment: -9.149, Acc:  22.0%\n",
      "Epoch 66/4000, AVG Loss:  6.906, AVG improvment: -14.806, Acc:  19.0%\n",
      "Epoch 67/4000, AVG Loss:  6.534, AVG improvment: -15.878, Acc:  18.0%\n",
      "Epoch 68/4000, AVG Loss:  11.101, AVG improvment: -4.032, Acc:  13.0%\n",
      "Epoch 69/4000, AVG Loss:  9.118, AVG improvment: -9.736, Acc:  13.0%\n",
      "Epoch 70/4000, AVG Loss:  10.975, AVG improvment: -5.433, Acc:  13.0%\n",
      "Epoch 71/4000, AVG Loss:  9.187, AVG improvment: -6.721, Acc:  18.0%\n",
      "Epoch 72/4000, AVG Loss:  12.411, AVG improvment: -4.001, Acc:  7.0%\n",
      "Epoch 73/4000, AVG Loss:  7.733, AVG improvment: -5.882, Acc:  10.0%\n",
      "Epoch 74/4000, AVG Loss:  6.979, AVG improvment: -4.943, Acc:  14.0%\n",
      "Epoch 75/4000, AVG Loss:  8.329, AVG improvment: -7.861, Acc:  16.0%\n",
      "Epoch 76/4000, AVG Loss:  8.145, AVG improvment: -9.159, Acc:  21.0%\n",
      "Epoch 77/4000, AVG Loss:  8.894, AVG improvment: -13.452, Acc:  21.0%\n",
      "Epoch 78/4000, AVG Loss:  7.628, AVG improvment: -8.827, Acc:  13.0%\n",
      "Epoch 79/4000, AVG Loss:  9.320, AVG improvment: -8.999, Acc:  15.0%\n",
      "Epoch 80/4000, AVG Loss:  7.248, AVG improvment: -7.619, Acc:  16.0%\n",
      "Epoch 81/4000, AVG Loss:  7.036, AVG improvment: -11.891, Acc:  29.0%\n",
      "Epoch 82/4000, AVG Loss:  7.815, AVG improvment: -11.667, Acc:  20.0%\n",
      "Epoch 83/4000, AVG Loss:  6.759, AVG improvment: -11.929, Acc:  22.0%\n",
      "Epoch 84/4000, AVG Loss:  7.757, AVG improvment: -11.891, Acc:  23.0%\n",
      "Epoch 85/4000, AVG Loss:  10.357, AVG improvment: -9.778, Acc:  26.0%\n",
      "Epoch 86/4000, AVG Loss:  10.650, AVG improvment: -11.660, Acc:  19.0%\n",
      "Epoch 87/4000, AVG Loss:  4.707, AVG improvment: -10.482, Acc:  18.0%\n",
      "Epoch 88/4000, AVG Loss:  9.912, AVG improvment: -10.609, Acc:  23.0%\n",
      "Epoch 89/4000, AVG Loss:  8.021, AVG improvment: -14.181, Acc:  36.0%\n",
      "Epoch 90/4000, AVG Loss:  6.940, AVG improvment: -17.114, Acc:  42.0%\n",
      "Epoch 91/4000, AVG Loss:  11.585, AVG improvment: -11.073, Acc:  24.0%\n",
      "Epoch 92/4000, AVG Loss:  5.927, AVG improvment: -18.918, Acc:  32.0%\n",
      "Epoch 93/4000, AVG Loss:  8.856, AVG improvment: -12.050, Acc:  31.0%\n",
      "Epoch 94/4000, AVG Loss:  7.110, AVG improvment: -13.308, Acc:  26.0%\n",
      "Epoch 95/4000, AVG Loss:  5.473, AVG improvment: -23.923, Acc:  42.0%\n",
      "------------------------ New best Dec with -10.048 ------------------------\n",
      "Epoch 96/4000, AVG Loss:  6.566, AVG improvment: -13.047, Acc:  30.0%\n",
      "Epoch 97/4000, AVG Loss:  7.544, AVG improvment: -14.216, Acc:  29.0%\n",
      "Epoch 98/4000, AVG Loss:  7.935, AVG improvment: -18.757, Acc:  29.0%\n",
      "Epoch 99/4000, AVG Loss:  8.169, AVG improvment: -13.571, Acc:  27.0%\n",
      "Epoch 100/4000, AVG Loss:  9.499, AVG improvment: -9.044, Acc:  23.0%\n",
      "Epoch 101/4000, AVG Loss:  7.125, AVG improvment: -12.797, Acc:  30.0%\n",
      "Modelsnapshot saved as:ModelRNNE100_L 7.12_I-12.80_P 0.30\n",
      "Epoch 102/4000, AVG Loss:  8.528, AVG improvment: -14.213, Acc:  30.0%\n",
      "Epoch 103/4000, AVG Loss:  6.857, AVG improvment: -13.985, Acc:  24.0%\n",
      "Epoch 104/4000, AVG Loss:  7.972, AVG improvment: -15.158, Acc:  21.0%\n",
      "Epoch 105/4000, AVG Loss:  7.277, AVG improvment: -10.633, Acc:  27.0%\n",
      "Epoch 106/4000, AVG Loss:  8.403, AVG improvment: -14.600, Acc:  31.0%\n",
      "Epoch 107/4000, AVG Loss:  10.642, AVG improvment: -22.835, Acc:  37.0%\n",
      "Epoch 108/4000, AVG Loss:  8.825, AVG improvment: -15.959, Acc:  27.0%\n",
      "Epoch 109/4000, AVG Loss:  6.125, AVG improvment: -17.346, Acc:  31.0%\n",
      "Epoch 110/4000, AVG Loss:  8.336, AVG improvment: -11.813, Acc:  23.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/4000, AVG Loss:  7.360, AVG improvment: -10.160, Acc:  23.0%\n",
      "Epoch 112/4000, AVG Loss:  6.332, AVG improvment: -12.541, Acc:  29.0%\n",
      "Epoch 113/4000, AVG Loss:  4.618, AVG improvment: -17.044, Acc:  28.0%\n",
      "Epoch 114/4000, AVG Loss:  5.257, AVG improvment: -17.318, Acc:  21.0%\n",
      "Epoch 115/4000, AVG Loss:  4.527, AVG improvment: -15.094, Acc:  20.0%\n",
      "Epoch 116/4000, AVG Loss:  3.590, AVG improvment: -16.982, Acc:  33.0%\n",
      "Epoch 117/4000, AVG Loss:  6.709, AVG improvment: -10.815, Acc:  28.0%\n",
      "Epoch 118/4000, AVG Loss:  8.819, AVG improvment: -11.911, Acc:  28.0%\n",
      "Epoch 119/4000, AVG Loss:  5.920, AVG improvment: -10.558, Acc:  20.0%\n",
      "Epoch 120/4000, AVG Loss:  7.984, AVG improvment: -13.186, Acc:  26.0%\n",
      "Epoch 121/4000, AVG Loss:  6.005, AVG improvment: -14.536, Acc:  20.0%\n",
      "Epoch 122/4000, AVG Loss:  5.076, AVG improvment: -17.264, Acc:  27.0%\n",
      "Epoch 123/4000, AVG Loss:  7.687, AVG improvment: -15.203, Acc:  24.0%\n",
      "Epoch 124/4000, AVG Loss:  4.088, AVG improvment: -12.705, Acc:  25.0%\n",
      "Epoch 125/4000, AVG Loss:  6.284, AVG improvment: -11.338, Acc:  31.0%\n",
      "Epoch 126/4000, AVG Loss:  7.578, AVG improvment: -11.120, Acc:  29.0%\n",
      "Epoch 127/4000, AVG Loss:  6.766, AVG improvment: -9.004, Acc:  25.0%\n",
      "Epoch 128/4000, AVG Loss:  7.850, AVG improvment: -13.579, Acc:  27.0%\n",
      "Epoch 129/4000, AVG Loss:  5.985, AVG improvment: -14.967, Acc:  31.0%\n",
      "Epoch 130/4000, AVG Loss:  4.471, AVG improvment: -16.797, Acc:  30.0%\n",
      "Epoch 131/4000, AVG Loss:  5.736, AVG improvment: -14.578, Acc:  25.0%\n",
      "Epoch 132/4000, AVG Loss:  4.650, AVG improvment: -16.153, Acc:  31.0%\n",
      "Epoch 133/4000, AVG Loss:  5.632, AVG improvment: -16.042, Acc:  29.0%\n",
      "Epoch 134/4000, AVG Loss:  6.837, AVG improvment: -17.768, Acc:  29.0%\n",
      "Epoch 135/4000, AVG Loss:  7.424, AVG improvment: -9.143, Acc:  26.0%\n",
      "Epoch 136/4000, AVG Loss:  7.900, AVG improvment: -13.097, Acc:  30.0%\n",
      "Epoch 137/4000, AVG Loss:  7.362, AVG improvment: -11.216, Acc:  24.0%\n",
      "Epoch 138/4000, AVG Loss:  8.926, AVG improvment: -9.926, Acc:  27.0%\n",
      "Epoch 139/4000, AVG Loss:  5.583, AVG improvment: -16.041, Acc:  30.0%\n",
      "Epoch 140/4000, AVG Loss:  6.796, AVG improvment: -7.643, Acc:  24.0%\n",
      "Epoch 141/4000, AVG Loss:  5.046, AVG improvment: -17.272, Acc:  27.0%\n",
      "Epoch 142/4000, AVG Loss:  7.581, AVG improvment: -11.207, Acc:  28.0%\n",
      "Epoch 143/4000, AVG Loss:  5.691, AVG improvment: -15.948, Acc:  28.0%\n",
      "Epoch 144/4000, AVG Loss:  7.268, AVG improvment: -13.111, Acc:  26.0%\n",
      "Epoch 145/4000, AVG Loss:  4.641, AVG improvment: -10.075, Acc:  20.0%\n",
      "Epoch 146/4000, AVG Loss:  7.560, AVG improvment: -10.412, Acc:  28.0%\n",
      "Epoch 147/4000, AVG Loss:  6.634, AVG improvment: -11.929, Acc:  30.0%\n",
      "Epoch 148/4000, AVG Loss:  5.749, AVG improvment: -14.259, Acc:  31.0%\n",
      "Epoch 149/4000, AVG Loss:  5.899, AVG improvment: -13.914, Acc:  23.0%\n",
      "Epoch 150/4000, AVG Loss:  7.055, AVG improvment: -13.413, Acc:  26.0%\n",
      "Epoch 151/4000, AVG Loss:  5.207, AVG improvment: -14.407, Acc:  28.0%\n",
      "Epoch 152/4000, AVG Loss:  6.255, AVG improvment: -12.493, Acc:  26.0%\n",
      "Epoch 153/4000, AVG Loss:  6.151, AVG improvment: -6.872, Acc:  23.0%\n",
      "Epoch 154/4000, AVG Loss:  5.198, AVG improvment: -14.331, Acc:  27.0%\n",
      "Epoch 155/4000, AVG Loss:  9.213, AVG improvment: -10.298, Acc:  23.0%\n",
      "Epoch 156/4000, AVG Loss:  7.349, AVG improvment: -8.772, Acc:  25.0%\n",
      "Epoch 157/4000, AVG Loss:  7.909, AVG improvment: -9.919, Acc:  24.0%\n",
      "Epoch 158/4000, AVG Loss:  6.530, AVG improvment: -10.459, Acc:  18.0%\n",
      "Epoch 159/4000, AVG Loss:  5.905, AVG improvment: -5.824, Acc:  19.0%\n",
      "Epoch 160/4000, AVG Loss:  6.609, AVG improvment: -13.831, Acc:  28.0%\n",
      "Epoch 161/4000, AVG Loss:  6.931, AVG improvment: -7.717, Acc:  18.0%\n",
      "Epoch 162/4000, AVG Loss:  8.139, AVG improvment: -13.657, Acc:  31.0%\n",
      "Epoch 163/4000, AVG Loss:  5.441, AVG improvment: -12.685, Acc:  27.0%\n",
      "Epoch 164/4000, AVG Loss:  7.000, AVG improvment: -12.644, Acc:  30.0%\n",
      "Epoch 165/4000, AVG Loss:  6.835, AVG improvment: -13.791, Acc:  26.0%\n",
      "Epoch 166/4000, AVG Loss:  5.742, AVG improvment: -15.545, Acc:  27.0%\n",
      "Epoch 167/4000, AVG Loss:  6.871, AVG improvment: -11.601, Acc:  25.0%\n",
      "Epoch 168/4000, AVG Loss:  4.420, AVG improvment: -16.734, Acc:  30.0%\n",
      "Epoch 169/4000, AVG Loss:  6.390, AVG improvment: -12.867, Acc:  34.0%\n",
      "Epoch 170/4000, AVG Loss:  5.355, AVG improvment: -12.148, Acc:  26.0%\n",
      "Epoch 171/4000, AVG Loss:  4.313, AVG improvment: -20.370, Acc:  33.0%\n",
      "Epoch 172/4000, AVG Loss:  7.361, AVG improvment: -17.411, Acc:  29.0%\n",
      "Epoch 173/4000, AVG Loss:  8.111, AVG improvment: -14.202, Acc:  30.0%\n",
      "Epoch 174/4000, AVG Loss:  8.773, AVG improvment: -9.707, Acc:  24.0%\n",
      "Epoch 175/4000, AVG Loss:  5.111, AVG improvment: -18.317, Acc:  31.0%\n",
      "Epoch 176/4000, AVG Loss:  7.837, AVG improvment: -10.217, Acc:  22.0%\n",
      "Epoch 177/4000, AVG Loss:  6.632, AVG improvment: -5.136, Acc:  12.0%\n",
      "Epoch 178/4000, AVG Loss:  7.257, AVG improvment: -9.310, Acc:  18.0%\n",
      "Epoch 179/4000, AVG Loss:  6.691, AVG improvment: -10.693, Acc:  25.0%\n",
      "Epoch 180/4000, AVG Loss:  4.315, AVG improvment: -14.699, Acc:  24.0%\n",
      "Epoch 181/4000, AVG Loss:  9.917, AVG improvment: -7.886, Acc:  20.0%\n",
      "Epoch 182/4000, AVG Loss:  8.701, AVG improvment: -10.311, Acc:  27.0%\n",
      "Epoch 183/4000, AVG Loss:  4.496, AVG improvment: -17.440, Acc:  32.0%\n",
      "Epoch 184/4000, AVG Loss:  8.615, AVG improvment: -8.409, Acc:  23.0%\n",
      "Epoch 185/4000, AVG Loss:  6.958, AVG improvment: -7.480, Acc:  18.0%\n",
      "Epoch 186/4000, AVG Loss:  5.913, AVG improvment: -10.533, Acc:  21.0%\n",
      "Epoch 187/4000, AVG Loss:  5.644, AVG improvment: -18.978, Acc:  35.0%\n",
      "Epoch 188/4000, AVG Loss:  4.766, AVG improvment: -15.395, Acc:  30.0%\n",
      "Epoch 189/4000, AVG Loss:  5.009, AVG improvment: -16.020, Acc:  27.0%\n",
      "Epoch 190/4000, AVG Loss:  4.566, AVG improvment: -23.451, Acc:  31.0%\n",
      "Epoch 191/4000, AVG Loss:  6.110, AVG improvment: -13.429, Acc:  24.0%\n",
      "Epoch 192/4000, AVG Loss:  7.791, AVG improvment: -10.731, Acc:  21.0%\n",
      "Epoch 193/4000, AVG Loss:  5.281, AVG improvment: -13.759, Acc:  29.0%\n",
      "Epoch 194/4000, AVG Loss:  6.874, AVG improvment: -8.242, Acc:  21.0%\n",
      "Epoch 195/4000, AVG Loss:  4.777, AVG improvment: -17.327, Acc:  32.0%\n",
      "Epoch 196/4000, AVG Loss:  7.555, AVG improvment: -14.796, Acc:  29.0%\n",
      "Epoch 197/4000, AVG Loss:  4.452, AVG improvment: -11.383, Acc:  26.0%\n",
      "Epoch 198/4000, AVG Loss:  6.205, AVG improvment: -13.045, Acc:  27.0%\n",
      "Epoch 199/4000, AVG Loss:  5.239, AVG improvment: -12.117, Acc:  27.0%\n",
      "Epoch 200/4000, AVG Loss:  5.281, AVG improvment: -16.837, Acc:  23.0%\n",
      "Epoch 201/4000, AVG Loss:  7.093, AVG improvment: -9.986, Acc:  22.0%\n",
      "Modelsnapshot saved as:ModelRNNE200_L 7.09_I-9.99_P 0.22\n",
      "Epoch 202/4000, AVG Loss:  5.543, AVG improvment: -15.735, Acc:  32.0%\n",
      "Epoch 203/4000, AVG Loss:  6.586, AVG improvment: -13.672, Acc:  20.0%\n",
      "Epoch 204/4000, AVG Loss:  7.738, AVG improvment: -11.283, Acc:  23.0%\n",
      "Epoch 205/4000, AVG Loss:  6.189, AVG improvment: -11.071, Acc:  24.0%\n",
      "Epoch 206/4000, AVG Loss:  4.147, AVG improvment: -14.980, Acc:  25.0%\n",
      "Epoch 207/4000, AVG Loss:  8.261, AVG improvment: -15.332, Acc:  26.0%\n",
      "Epoch 208/4000, AVG Loss:  5.445, AVG improvment: -12.717, Acc:  21.0%\n",
      "Epoch 209/4000, AVG Loss:  7.728, AVG improvment: -13.178, Acc:  24.0%\n",
      "Epoch 210/4000, AVG Loss:  7.027, AVG improvment: -9.456, Acc:  23.0%\n",
      "Epoch 211/4000, AVG Loss:  7.109, AVG improvment: -13.276, Acc:  20.0%\n",
      "Epoch 212/4000, AVG Loss:  6.147, AVG improvment: -12.407, Acc:  28.0%\n",
      "Epoch 213/4000, AVG Loss:  5.484, AVG improvment: -12.143, Acc:  27.0%\n",
      "Epoch 214/4000, AVG Loss:  6.228, AVG improvment: -8.389, Acc:  17.0%\n",
      "Epoch 215/4000, AVG Loss:  6.059, AVG improvment: -20.054, Acc:  34.0%\n",
      "Epoch 216/4000, AVG Loss:  4.644, AVG improvment: -13.941, Acc:  28.0%\n",
      "Epoch 217/4000, AVG Loss:  5.477, AVG improvment: -13.720, Acc:  27.0%\n",
      "Epoch 218/4000, AVG Loss:  6.785, AVG improvment: -16.347, Acc:  33.0%\n",
      "Epoch 219/4000, AVG Loss:  5.060, AVG improvment: -19.777, Acc:  42.0%\n",
      "Epoch 220/4000, AVG Loss:  3.810, AVG improvment: -16.503, Acc:  31.0%\n",
      "Epoch 221/4000, AVG Loss:  4.581, AVG improvment: -18.243, Acc:  30.0%\n",
      "Epoch 222/4000, AVG Loss:  4.290, AVG improvment: -15.410, Acc:  31.0%\n",
      "Epoch 223/4000, AVG Loss:  3.340, AVG improvment: -18.488, Acc:  27.0%\n",
      "Epoch 224/4000, AVG Loss:  4.298, AVG improvment: -9.905, Acc:  20.0%\n",
      "Epoch 225/4000, AVG Loss:  5.794, AVG improvment: -13.159, Acc:  27.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 226/4000, AVG Loss:  6.341, AVG improvment: -9.781, Acc:  28.0%\n",
      "Epoch 227/4000, AVG Loss:  5.024, AVG improvment: -15.290, Acc:  33.0%\n",
      "Epoch 228/4000, AVG Loss:  4.808, AVG improvment: -18.919, Acc:  36.0%\n",
      "Epoch 229/4000, AVG Loss:  5.427, AVG improvment: -14.363, Acc:  29.0%\n",
      "Epoch 230/4000, AVG Loss:  5.198, AVG improvment: -16.964, Acc:  33.0%\n",
      "Epoch 231/4000, AVG Loss:  5.894, AVG improvment: -16.490, Acc:  33.0%\n",
      "Epoch 232/4000, AVG Loss:  6.196, AVG improvment: -14.024, Acc:  26.0%\n",
      "Epoch 233/4000, AVG Loss:  5.229, AVG improvment: -10.317, Acc:  20.0%\n",
      "Epoch 234/4000, AVG Loss:  5.024, AVG improvment: -12.711, Acc:  27.0%\n",
      "Epoch 235/4000, AVG Loss:  6.494, AVG improvment: -9.282, Acc:  25.0%\n",
      "Epoch 236/4000, AVG Loss:  6.220, AVG improvment: -17.939, Acc:  33.0%\n",
      "Epoch 237/4000, AVG Loss:  3.932, AVG improvment: -15.084, Acc:  29.0%\n",
      "Epoch 238/4000, AVG Loss:  6.659, AVG improvment: -11.840, Acc:  24.0%\n",
      "Epoch 239/4000, AVG Loss:  5.579, AVG improvment: -11.953, Acc:  30.0%\n",
      "Epoch 240/4000, AVG Loss:  5.355, AVG improvment: -16.099, Acc:  38.0%\n",
      "Epoch 241/4000, AVG Loss:  5.049, AVG improvment: -14.278, Acc:  34.0%\n",
      "Epoch 242/4000, AVG Loss:  6.078, AVG improvment: -14.507, Acc:  33.0%\n",
      "Epoch 243/4000, AVG Loss:  6.270, AVG improvment: -10.996, Acc:  27.0%\n",
      "Epoch 244/4000, AVG Loss:  6.851, AVG improvment: -12.977, Acc:  26.0%\n",
      "Epoch 245/4000, AVG Loss:  6.374, AVG improvment: -11.901, Acc:  33.0%\n",
      "Epoch 246/4000, AVG Loss:  4.353, AVG improvment: -14.880, Acc:  42.0%\n",
      "Epoch 247/4000, AVG Loss:  7.141, AVG improvment: -10.526, Acc:  31.0%\n",
      "Epoch 248/4000, AVG Loss:  6.166, AVG improvment: -9.612, Acc:  22.0%\n",
      "Epoch 249/4000, AVG Loss:  6.026, AVG improvment: -13.883, Acc:  34.0%\n",
      "Epoch 250/4000, AVG Loss:  5.276, AVG improvment: -17.065, Acc:  31.0%\n",
      "Epoch 251/4000, AVG Loss:  6.032, AVG improvment: -16.585, Acc:  36.0%\n",
      "Epoch 252/4000, AVG Loss:  5.588, AVG improvment: -12.219, Acc:  28.0%\n",
      "Epoch 253/4000, AVG Loss:  4.889, AVG improvment: -15.289, Acc:  37.0%\n",
      "Epoch 254/4000, AVG Loss:  6.463, AVG improvment: -15.793, Acc:  34.0%\n",
      "Epoch 255/4000, AVG Loss:  5.970, AVG improvment: -10.410, Acc:  30.0%\n",
      "Epoch 256/4000, AVG Loss:  4.406, AVG improvment: -19.718, Acc:  34.0%\n",
      "Epoch 257/4000, AVG Loss:  3.552, AVG improvment: -18.782, Acc:  37.0%\n",
      "Epoch 258/4000, AVG Loss:  4.547, AVG improvment: -19.224, Acc:  34.0%\n",
      "Epoch 259/4000, AVG Loss:  5.513, AVG improvment: -13.568, Acc:  30.0%\n",
      "Epoch 260/4000, AVG Loss:  5.246, AVG improvment: -15.390, Acc:  30.0%\n",
      "Epoch 261/4000, AVG Loss:  4.099, AVG improvment: -18.542, Acc:  35.0%\n",
      "Epoch 262/4000, AVG Loss:  3.591, AVG improvment: -18.375, Acc:  33.0%\n",
      "Epoch 263/4000, AVG Loss:  4.484, AVG improvment: -12.867, Acc:  34.0%\n",
      "Epoch 264/4000, AVG Loss:  6.524, AVG improvment: -15.290, Acc:  33.0%\n",
      "Epoch 265/4000, AVG Loss:  4.635, AVG improvment: -21.852, Acc:  31.0%\n",
      "Epoch 266/4000, AVG Loss:  6.333, AVG improvment: -12.566, Acc:  31.0%\n",
      "Epoch 267/4000, AVG Loss:  3.976, AVG improvment: -13.913, Acc:  31.0%\n",
      "Epoch 268/4000, AVG Loss:  4.087, AVG improvment: -23.108, Acc:  40.0%\n",
      "Epoch 269/4000, AVG Loss:  4.762, AVG improvment: -17.360, Acc:  34.0%\n",
      "Epoch 270/4000, AVG Loss:  5.378, AVG improvment: -13.729, Acc:  32.0%\n",
      "Epoch 271/4000, AVG Loss:  6.678, AVG improvment: -10.659, Acc:  31.0%\n",
      "Epoch 272/4000, AVG Loss:  7.882, AVG improvment: -14.751, Acc:  38.0%\n",
      "Epoch 273/4000, AVG Loss:  4.200, AVG improvment: -18.482, Acc:  35.0%\n",
      "Epoch 274/4000, AVG Loss:  6.461, AVG improvment: -12.295, Acc:  33.0%\n",
      "Epoch 275/4000, AVG Loss:  5.847, AVG improvment: -13.792, Acc:  33.0%\n",
      "Epoch 276/4000, AVG Loss:  4.789, AVG improvment: -24.537, Acc:  34.0%\n",
      "Epoch 277/4000, AVG Loss:  5.240, AVG improvment: -19.401, Acc:  35.0%\n",
      "Epoch 278/4000, AVG Loss:  4.155, AVG improvment: -21.255, Acc:  34.0%\n",
      "Epoch 279/4000, AVG Loss:  6.275, AVG improvment: -16.265, Acc:  29.0%\n",
      "Epoch 280/4000, AVG Loss:  4.842, AVG improvment: -18.534, Acc:  33.0%\n",
      "Epoch 281/4000, AVG Loss:  2.410, AVG improvment: -20.910, Acc:  41.0%\n",
      "Epoch 282/4000, AVG Loss:  4.881, AVG improvment: -19.648, Acc:  41.0%\n",
      "Epoch 283/4000, AVG Loss:  4.424, AVG improvment: -19.785, Acc:  35.0%\n",
      "Epoch 284/4000, AVG Loss:  3.898, AVG improvment: -19.592, Acc:  39.0%\n",
      "Epoch 285/4000, AVG Loss:  6.143, AVG improvment: -17.965, Acc:  40.0%\n",
      "Epoch 286/4000, AVG Loss:  4.809, AVG improvment: -16.477, Acc:  34.0%\n",
      "Epoch 287/4000, AVG Loss:  3.648, AVG improvment: -24.099, Acc:  38.0%\n",
      "Epoch 288/4000, AVG Loss:  4.630, AVG improvment: -25.165, Acc:  43.0%\n",
      "------------------------ New best Dec with -10.821 ------------------------\n",
      "Epoch 289/4000, AVG Loss:  5.778, AVG improvment: -15.906, Acc:  38.0%\n",
      "Epoch 290/4000, AVG Loss:  2.393, AVG improvment: -27.639, Acc:  48.0%\n",
      "------------------------ New best Dec with -13.267 ------------------------\n",
      "Epoch 291/4000, AVG Loss:  4.413, AVG improvment: -15.492, Acc:  29.0%\n",
      "Epoch 292/4000, AVG Loss:  5.539, AVG improvment: -16.642, Acc:  37.0%\n",
      "Epoch 293/4000, AVG Loss:  4.035, AVG improvment: -19.751, Acc:  36.0%\n",
      "Epoch 294/4000, AVG Loss:  3.213, AVG improvment: -21.137, Acc:  39.0%\n",
      "Epoch 295/4000, AVG Loss:  2.269, AVG improvment: -27.216, Acc:  50.0%\n",
      "------------------------ New best Dec with -13.608 ------------------------\n",
      "Epoch 296/4000, AVG Loss:  5.416, AVG improvment: -14.608, Acc:  33.0%\n",
      "Epoch 297/4000, AVG Loss:  5.355, AVG improvment: -16.216, Acc:  36.0%\n",
      "Epoch 298/4000, AVG Loss:  5.057, AVG improvment: -20.335, Acc:  34.0%\n",
      "Epoch 299/4000, AVG Loss:  4.750, AVG improvment: -17.470, Acc:  34.0%\n",
      "Epoch 300/4000, AVG Loss:  4.098, AVG improvment: -18.259, Acc:  39.0%\n",
      "Epoch 301/4000, AVG Loss:  4.452, AVG improvment: -15.114, Acc:  34.0%\n",
      "Modelsnapshot saved as:ModelRNNE300_L 4.45_I-15.11_P 0.34\n",
      "Epoch 302/4000, AVG Loss:  5.153, AVG improvment: -16.303, Acc:  36.0%\n",
      "Epoch 303/4000, AVG Loss:  4.429, AVG improvment: -15.967, Acc:  35.0%\n",
      "Epoch 304/4000, AVG Loss:  7.260, AVG improvment: -11.078, Acc:  24.0%\n",
      "Epoch 305/4000, AVG Loss:  5.528, AVG improvment: -16.689, Acc:  28.0%\n",
      "Epoch 306/4000, AVG Loss:  4.781, AVG improvment: -22.279, Acc:  42.0%\n",
      "Epoch 307/4000, AVG Loss:  5.028, AVG improvment: -18.095, Acc:  34.0%\n",
      "Epoch 308/4000, AVG Loss:  6.231, AVG improvment: -12.098, Acc:  32.0%\n",
      "Epoch 309/4000, AVG Loss:  6.035, AVG improvment: -17.649, Acc:  32.0%\n",
      "Epoch 310/4000, AVG Loss:  7.049, AVG improvment: -18.945, Acc:  37.0%\n",
      "Epoch 311/4000, AVG Loss:  6.017, AVG improvment: -12.324, Acc:  31.0%\n",
      "Epoch 312/4000, AVG Loss:  4.805, AVG improvment: -15.769, Acc:  29.0%\n",
      "Epoch 313/4000, AVG Loss:  3.857, AVG improvment: -15.018, Acc:  31.0%\n",
      "Epoch 314/4000, AVG Loss:  4.748, AVG improvment: -16.005, Acc:  31.0%\n",
      "Epoch 315/4000, AVG Loss:  3.958, AVG improvment: -12.767, Acc:  27.0%\n",
      "Epoch 316/4000, AVG Loss:  3.758, AVG improvment: -19.228, Acc:  39.0%\n",
      "Epoch 317/4000, AVG Loss:  2.965, AVG improvment: -18.320, Acc:  42.0%\n",
      "Epoch 318/4000, AVG Loss:  4.283, AVG improvment: -17.058, Acc:  31.0%\n",
      "Epoch 319/4000, AVG Loss:  4.762, AVG improvment: -15.266, Acc:  31.0%\n",
      "Epoch 320/4000, AVG Loss:  4.246, AVG improvment: -13.697, Acc:  27.0%\n",
      "Epoch 321/4000, AVG Loss:  3.994, AVG improvment: -19.594, Acc:  33.0%\n",
      "Epoch 322/4000, AVG Loss:  3.400, AVG improvment: -14.708, Acc:  30.0%\n",
      "Epoch 323/4000, AVG Loss:  5.148, AVG improvment: -12.840, Acc:  30.0%\n",
      "Epoch 324/4000, AVG Loss:  3.265, AVG improvment: -12.523, Acc:  33.0%\n",
      "Epoch 325/4000, AVG Loss:  5.699, AVG improvment: -16.901, Acc:  38.0%\n",
      "Epoch 326/4000, AVG Loss:  3.949, AVG improvment: -20.668, Acc:  40.0%\n",
      "Epoch 327/4000, AVG Loss:  4.572, AVG improvment: -19.861, Acc:  43.0%\n",
      "Epoch 328/4000, AVG Loss:  2.974, AVG improvment: -16.936, Acc:  28.0%\n",
      "Epoch 329/4000, AVG Loss:  4.767, AVG improvment: -22.854, Acc:  45.0%\n",
      "Epoch 330/4000, AVG Loss:  2.482, AVG improvment: -23.697, Acc:  42.0%\n",
      "Epoch 331/4000, AVG Loss:  6.285, AVG improvment: -16.690, Acc:  32.0%\n",
      "Epoch 332/4000, AVG Loss:  3.037, AVG improvment: -22.816, Acc:  38.0%\n",
      "Epoch 333/4000, AVG Loss:  4.865, AVG improvment: -16.715, Acc:  30.0%\n",
      "Epoch 334/4000, AVG Loss:  5.481, AVG improvment: -12.881, Acc:  34.0%\n",
      "Epoch 335/4000, AVG Loss:  5.246, AVG improvment: -13.533, Acc:  31.0%\n",
      "Epoch 336/4000, AVG Loss:  4.210, AVG improvment: -19.140, Acc:  31.0%\n",
      "Epoch 337/4000, AVG Loss:  4.480, AVG improvment: -14.009, Acc:  32.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 338/4000, AVG Loss:  5.035, AVG improvment: -19.030, Acc:  42.0%\n",
      "Epoch 339/4000, AVG Loss:  3.940, AVG improvment: -20.026, Acc:  34.0%\n",
      "Epoch 340/4000, AVG Loss:  4.903, AVG improvment: -20.243, Acc:  45.0%\n",
      "Epoch 341/4000, AVG Loss:  5.294, AVG improvment: -19.347, Acc:  41.0%\n",
      "Epoch 342/4000, AVG Loss:  5.798, AVG improvment: -10.892, Acc:  30.0%\n",
      "Epoch 343/4000, AVG Loss:  5.959, AVG improvment: -13.554, Acc:  36.0%\n",
      "Epoch 344/4000, AVG Loss:  4.423, AVG improvment: -14.162, Acc:  37.0%\n",
      "Epoch 345/4000, AVG Loss:  6.272, AVG improvment: -13.855, Acc:  36.0%\n",
      "Epoch 346/4000, AVG Loss:  3.693, AVG improvment: -21.090, Acc:  32.0%\n",
      "Epoch 347/4000, AVG Loss:  4.930, AVG improvment: -13.398, Acc:  36.0%\n",
      "Epoch 348/4000, AVG Loss:  4.830, AVG improvment: -10.400, Acc:  27.0%\n",
      "Epoch 349/4000, AVG Loss:  4.199, AVG improvment: -21.206, Acc:  40.0%\n",
      "Epoch 350/4000, AVG Loss:  2.918, AVG improvment: -23.899, Acc:  38.0%\n",
      "Epoch 351/4000, AVG Loss:  3.711, AVG improvment: -20.439, Acc:  31.0%\n",
      "Epoch 352/4000, AVG Loss:  4.927, AVG improvment: -17.948, Acc:  35.0%\n",
      "Epoch 353/4000, AVG Loss:  4.640, AVG improvment: -14.698, Acc:  33.0%\n",
      "Epoch 354/4000, AVG Loss:  4.619, AVG improvment: -14.180, Acc:  40.0%\n",
      "Epoch 355/4000, AVG Loss:  3.586, AVG improvment: -12.739, Acc:  31.0%\n",
      "Epoch 356/4000, AVG Loss:  4.109, AVG improvment: -23.946, Acc:  36.0%\n",
      "Epoch 357/4000, AVG Loss:  5.647, AVG improvment: -20.525, Acc:  37.0%\n",
      "Epoch 358/4000, AVG Loss:  4.371, AVG improvment: -16.941, Acc:  36.0%\n",
      "Epoch 359/4000, AVG Loss:  4.161, AVG improvment: -13.368, Acc:  28.0%\n",
      "Epoch 360/4000, AVG Loss:  4.380, AVG improvment: -12.800, Acc:  37.0%\n",
      "Epoch 361/4000, AVG Loss:  3.440, AVG improvment: -26.474, Acc:  40.0%\n",
      "Epoch 362/4000, AVG Loss:  5.060, AVG improvment: -14.785, Acc:  31.0%\n",
      "Epoch 363/4000, AVG Loss:  3.617, AVG improvment: -17.169, Acc:  37.0%\n",
      "Epoch 364/4000, AVG Loss:  3.503, AVG improvment: -16.580, Acc:  36.0%\n",
      "Epoch 365/4000, AVG Loss:  4.464, AVG improvment: -12.219, Acc:  34.0%\n",
      "Epoch 366/4000, AVG Loss:  7.146, AVG improvment: -9.814, Acc:  25.0%\n",
      "Epoch 367/4000, AVG Loss:  4.094, AVG improvment: -24.128, Acc:  45.0%\n",
      "Epoch 368/4000, AVG Loss:  5.034, AVG improvment: -10.644, Acc:  35.0%\n",
      "Epoch 369/4000, AVG Loss:  4.723, AVG improvment: -12.000, Acc:  37.0%\n",
      "Epoch 370/4000, AVG Loss:  5.552, AVG improvment: -14.950, Acc:  36.0%\n",
      "Epoch 371/4000, AVG Loss:  3.957, AVG improvment: -18.122, Acc:  34.0%\n",
      "Epoch 372/4000, AVG Loss:  5.758, AVG improvment: -17.176, Acc:  41.0%\n",
      "Epoch 373/4000, AVG Loss:  4.836, AVG improvment: -12.035, Acc:  32.0%\n",
      "Epoch 374/4000, AVG Loss:  4.949, AVG improvment: -13.031, Acc:  29.0%\n",
      "Epoch 375/4000, AVG Loss:  4.974, AVG improvment: -19.550, Acc:  39.0%\n",
      "Epoch 376/4000, AVG Loss:  3.459, AVG improvment: -20.867, Acc:  38.0%\n",
      "Epoch 377/4000, AVG Loss:  4.890, AVG improvment: -12.288, Acc:  28.0%\n",
      "Epoch 378/4000, AVG Loss:  5.846, AVG improvment: -17.144, Acc:  31.0%\n",
      "Epoch 379/4000, AVG Loss:  2.235, AVG improvment: -18.639, Acc:  37.0%\n",
      "Epoch 380/4000, AVG Loss:  7.177, AVG improvment: -12.123, Acc:  28.0%\n",
      "Epoch 381/4000, AVG Loss:  5.778, AVG improvment: -22.821, Acc:  34.0%\n",
      "Epoch 382/4000, AVG Loss:  4.992, AVG improvment: -15.861, Acc:  38.0%\n",
      "Epoch 383/4000, AVG Loss:  3.717, AVG improvment: -16.379, Acc:  38.0%\n",
      "Epoch 384/4000, AVG Loss:  4.114, AVG improvment: -17.132, Acc:  36.0%\n",
      "Epoch 385/4000, AVG Loss:  4.873, AVG improvment: -18.949, Acc:  34.0%\n",
      "Epoch 386/4000, AVG Loss:  5.028, AVG improvment: -17.865, Acc:  34.0%\n",
      "Epoch 387/4000, AVG Loss:  4.556, AVG improvment: -18.568, Acc:  36.0%\n",
      "Epoch 388/4000, AVG Loss:  3.565, AVG improvment: -19.221, Acc:  36.0%\n",
      "Epoch 389/4000, AVG Loss:  4.097, AVG improvment: -17.923, Acc:  35.0%\n",
      "Epoch 390/4000, AVG Loss:  3.344, AVG improvment: -22.058, Acc:  37.0%\n",
      "Epoch 391/4000, AVG Loss:  3.576, AVG improvment: -20.237, Acc:  40.0%\n",
      "Epoch 392/4000, AVG Loss:  4.114, AVG improvment: -17.874, Acc:  32.0%\n",
      "Epoch 393/4000, AVG Loss:  3.119, AVG improvment: -23.655, Acc:  44.0%\n",
      "Epoch 394/4000, AVG Loss:  3.157, AVG improvment: -28.931, Acc:  54.0%\n",
      "------------------------ New best Dec with -15.623 ------------------------\n",
      "Epoch 395/4000, AVG Loss:  2.559, AVG improvment: -25.281, Acc:  40.0%\n",
      "Epoch 396/4000, AVG Loss:  4.311, AVG improvment: -24.505, Acc:  43.0%\n",
      "Epoch 397/4000, AVG Loss:  5.416, AVG improvment: -13.184, Acc:  28.0%\n",
      "Epoch 398/4000, AVG Loss:  5.347, AVG improvment: -10.900, Acc:  30.0%\n",
      "Epoch 399/4000, AVG Loss:  5.364, AVG improvment: -19.001, Acc:  32.0%\n",
      "Epoch 400/4000, AVG Loss:  4.178, AVG improvment: -17.518, Acc:  34.0%\n",
      "Epoch 401/4000, AVG Loss:  3.166, AVG improvment: -14.900, Acc:  37.0%\n",
      "Modelsnapshot saved as:ModelRNNE400_L 3.17_I-14.90_P 0.37\n",
      "Epoch 402/4000, AVG Loss:  4.192, AVG improvment: -12.580, Acc:  38.0%\n",
      "Epoch 403/4000, AVG Loss:  4.799, AVG improvment: -18.263, Acc:  35.0%\n",
      "Epoch 404/4000, AVG Loss:  2.071, AVG improvment: -24.767, Acc:  41.0%\n",
      "Epoch 405/4000, AVG Loss:  3.130, AVG improvment: -13.355, Acc:  42.0%\n",
      "Epoch 406/4000, AVG Loss:  4.979, AVG improvment: -20.044, Acc:  39.0%\n",
      "Epoch 407/4000, AVG Loss:  5.767, AVG improvment: -11.094, Acc:  31.0%\n",
      "Epoch 408/4000, AVG Loss:  2.893, AVG improvment: -15.146, Acc:  26.0%\n",
      "Epoch 409/4000, AVG Loss:  4.249, AVG improvment: -11.227, Acc:  32.0%\n",
      "Epoch 410/4000, AVG Loss:  4.511, AVG improvment: -9.387, Acc:  29.0%\n",
      "Epoch 411/4000, AVG Loss:  4.223, AVG improvment: -12.249, Acc:  35.0%\n",
      "Epoch 412/4000, AVG Loss:  3.453, AVG improvment: -15.119, Acc:  29.0%\n",
      "Epoch 413/4000, AVG Loss:  3.332, AVG improvment: -12.399, Acc:  39.0%\n",
      "Epoch 414/4000, AVG Loss:  4.362, AVG improvment: -12.243, Acc:  29.0%\n",
      "Epoch 415/4000, AVG Loss:  4.299, AVG improvment: -10.532, Acc:  26.0%\n",
      "Epoch 416/4000, AVG Loss:  5.104, AVG improvment: -14.558, Acc:  30.0%\n",
      "Epoch 417/4000, AVG Loss:  5.152, AVG improvment: -9.371, Acc:  29.0%\n",
      "Epoch 418/4000, AVG Loss:  4.778, AVG improvment: -10.837, Acc:  32.0%\n",
      "Epoch 419/4000, AVG Loss:  3.456, AVG improvment: -9.738, Acc:  24.0%\n",
      "Epoch 420/4000, AVG Loss:  4.885, AVG improvment: -11.432, Acc:  29.0%\n",
      "Epoch 421/4000, AVG Loss:  5.713, AVG improvment: -15.063, Acc:  35.0%\n",
      "Epoch 422/4000, AVG Loss:  4.577, AVG improvment: -13.663, Acc:  29.0%\n",
      "Epoch 423/4000, AVG Loss:  3.450, AVG improvment: -12.530, Acc:  32.0%\n",
      "Epoch 424/4000, AVG Loss:  3.297, AVG improvment: -14.704, Acc:  31.0%\n",
      "Epoch 425/4000, AVG Loss:  3.810, AVG improvment: -15.672, Acc:  31.0%\n",
      "Epoch 426/4000, AVG Loss:  4.993, AVG improvment: -11.016, Acc:  28.0%\n",
      "Epoch 427/4000, AVG Loss:  4.112, AVG improvment: -19.160, Acc:  30.0%\n",
      "Epoch 428/4000, AVG Loss:  5.935, AVG improvment: -11.864, Acc:  25.0%\n",
      "Epoch 429/4000, AVG Loss:  3.153, AVG improvment: -14.666, Acc:  23.0%\n",
      "Epoch 430/4000, AVG Loss:  3.636, AVG improvment: -14.981, Acc:  31.0%\n",
      "Epoch 431/4000, AVG Loss:  3.836, AVG improvment: -23.795, Acc:  42.0%\n",
      "Epoch 432/4000, AVG Loss:  4.378, AVG improvment: -14.470, Acc:  39.0%\n",
      "Epoch 433/4000, AVG Loss:  4.740, AVG improvment: -10.784, Acc:  26.0%\n",
      "Epoch 434/4000, AVG Loss:  4.190, AVG improvment: -16.027, Acc:  32.0%\n",
      "Epoch 435/4000, AVG Loss:  5.550, AVG improvment: -16.336, Acc:  38.0%\n",
      "Epoch 436/4000, AVG Loss:  4.071, AVG improvment: -15.550, Acc:  29.0%\n",
      "Epoch 437/4000, AVG Loss:  3.762, AVG improvment: -17.059, Acc:  29.0%\n",
      "Epoch 438/4000, AVG Loss:  3.908, AVG improvment: -15.682, Acc:  33.0%\n",
      "Epoch 439/4000, AVG Loss:  3.515, AVG improvment: -12.232, Acc:  23.0%\n",
      "Epoch 440/4000, AVG Loss:  3.285, AVG improvment: -11.957, Acc:  27.0%\n",
      "Epoch 441/4000, AVG Loss:  4.499, AVG improvment: -11.098, Acc:  19.0%\n",
      "Epoch 442/4000, AVG Loss:  5.653, AVG improvment: -11.574, Acc:  24.0%\n",
      "Epoch 443/4000, AVG Loss:  4.957, AVG improvment: -11.860, Acc:  33.0%\n",
      "Epoch 444/4000, AVG Loss:  4.552, AVG improvment: -18.579, Acc:  43.0%\n",
      "Epoch 445/4000, AVG Loss:  4.961, AVG improvment: -13.995, Acc:  39.0%\n",
      "Epoch 446/4000, AVG Loss:  2.060, AVG improvment: -18.757, Acc:  38.0%\n",
      "Epoch 447/4000, AVG Loss:  4.180, AVG improvment: -9.253, Acc:  30.0%\n",
      "Epoch 448/4000, AVG Loss:  6.067, AVG improvment: -6.989, Acc:  30.0%\n",
      "Epoch 449/4000, AVG Loss:  5.094, AVG improvment: -9.117, Acc:  25.0%\n",
      "Epoch 450/4000, AVG Loss:  4.952, AVG improvment: -15.626, Acc:  30.0%\n",
      "Epoch 451/4000, AVG Loss:  4.570, AVG improvment: -12.626, Acc:  40.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 452/4000, AVG Loss:  4.060, AVG improvment: -16.823, Acc:  38.0%\n",
      "Epoch 453/4000, AVG Loss:  3.552, AVG improvment: -17.002, Acc:  39.0%\n",
      "Epoch 454/4000, AVG Loss:  4.554, AVG improvment: -7.231, Acc:  28.0%\n",
      "Epoch 455/4000, AVG Loss:  5.351, AVG improvment: -12.709, Acc:  38.0%\n",
      "Epoch 456/4000, AVG Loss:  4.604, AVG improvment: -15.992, Acc:  37.0%\n",
      "Epoch 457/4000, AVG Loss:  4.568, AVG improvment: -15.125, Acc:  36.0%\n",
      "Epoch 458/4000, AVG Loss:  3.334, AVG improvment: -12.296, Acc:  30.0%\n",
      "Epoch 459/4000, AVG Loss:  3.998, AVG improvment: -18.748, Acc:  33.0%\n",
      "Epoch 460/4000, AVG Loss:  5.741, AVG improvment: -14.477, Acc:  26.0%\n",
      "Epoch 461/4000, AVG Loss:  2.850, AVG improvment: -22.062, Acc:  35.0%\n",
      "Epoch 462/4000, AVG Loss:  5.511, AVG improvment: -12.403, Acc:  23.0%\n",
      "Epoch 463/4000, AVG Loss:  3.810, AVG improvment: -9.797, Acc:  22.0%\n",
      "Epoch 464/4000, AVG Loss:  4.786, AVG improvment: -13.372, Acc:  29.0%\n",
      "Epoch 465/4000, AVG Loss:  5.033, AVG improvment: -14.712, Acc:  26.0%\n",
      "Epoch 466/4000, AVG Loss:  2.642, AVG improvment: -16.268, Acc:  39.0%\n",
      "Epoch 467/4000, AVG Loss:  4.252, AVG improvment: -11.669, Acc:  31.0%\n",
      "Epoch 468/4000, AVG Loss:  4.287, AVG improvment: -17.912, Acc:  38.0%\n",
      "Epoch 469/4000, AVG Loss:  3.482, AVG improvment: -20.767, Acc:  42.0%\n",
      "Epoch 470/4000, AVG Loss:  3.645, AVG improvment: -19.285, Acc:  42.0%\n",
      "Epoch 471/4000, AVG Loss:  4.575, AVG improvment: -23.757, Acc:  38.0%\n",
      "Epoch 472/4000, AVG Loss:  3.413, AVG improvment: -18.372, Acc:  38.0%\n",
      "Epoch 473/4000, AVG Loss:  3.530, AVG improvment: -11.014, Acc:  28.0%\n",
      "Epoch 474/4000, AVG Loss:  3.671, AVG improvment: -16.238, Acc:  37.0%\n",
      "Epoch 475/4000, AVG Loss:  4.295, AVG improvment: -11.553, Acc:  33.0%\n",
      "Epoch 476/4000, AVG Loss:  4.357, AVG improvment: -16.615, Acc:  34.0%\n",
      "Epoch 477/4000, AVG Loss:  3.461, AVG improvment: -21.853, Acc:  41.0%\n",
      "Epoch 478/4000, AVG Loss:  5.701, AVG improvment: -10.860, Acc:  30.0%\n",
      "Epoch 479/4000, AVG Loss:  4.408, AVG improvment: -20.264, Acc:  40.0%\n",
      "Epoch 480/4000, AVG Loss:  3.340, AVG improvment: -21.581, Acc:  41.0%\n",
      "Epoch 481/4000, AVG Loss:  5.261, AVG improvment: -19.790, Acc:  33.0%\n",
      "Epoch 482/4000, AVG Loss:  3.229, AVG improvment: -15.938, Acc:  40.0%\n",
      "Epoch 483/4000, AVG Loss:  3.712, AVG improvment: -17.979, Acc:  34.0%\n",
      "Epoch 484/4000, AVG Loss:  3.909, AVG improvment: -15.192, Acc:  35.0%\n",
      "Epoch 485/4000, AVG Loss:  3.079, AVG improvment: -25.415, Acc:  39.0%\n",
      "Epoch 486/4000, AVG Loss:  4.328, AVG improvment: -11.185, Acc:  36.0%\n",
      "Epoch 487/4000, AVG Loss:  5.011, AVG improvment: -14.440, Acc:  34.0%\n",
      "Epoch 488/4000, AVG Loss:  3.740, AVG improvment: -18.654, Acc:  44.0%\n",
      "Epoch 489/4000, AVG Loss:  4.565, AVG improvment: -19.304, Acc:  50.0%\n",
      "Epoch 490/4000, AVG Loss:  3.910, AVG improvment: -17.392, Acc:  44.0%\n",
      "Epoch 491/4000, AVG Loss:  3.727, AVG improvment: -20.163, Acc:  46.0%\n",
      "Epoch 492/4000, AVG Loss:  4.201, AVG improvment: -14.302, Acc:  35.0%\n",
      "Epoch 493/4000, AVG Loss:  3.422, AVG improvment: -20.350, Acc:  44.0%\n",
      "Epoch 494/4000, AVG Loss:  2.480, AVG improvment: -25.716, Acc:  44.0%\n",
      "Epoch 495/4000, AVG Loss:  3.549, AVG improvment: -20.388, Acc:  40.0%\n",
      "Epoch 496/4000, AVG Loss:  3.808, AVG improvment: -17.057, Acc:  37.0%\n",
      "Epoch 497/4000, AVG Loss:  2.244, AVG improvment: -24.204, Acc:  41.0%\n",
      "Epoch 498/4000, AVG Loss:  4.417, AVG improvment: -18.465, Acc:  44.0%\n",
      "Epoch 499/4000, AVG Loss:  4.485, AVG improvment: -15.482, Acc:  35.0%\n",
      "Epoch 500/4000, AVG Loss:  3.982, AVG improvment: -21.501, Acc:  42.0%\n",
      "Epoch 501/4000, AVG Loss:  4.635, AVG improvment: -19.519, Acc:  32.0%\n",
      "Modelsnapshot saved as:ModelRNNE500_L 4.64_I-19.52_P 0.32\n",
      "Epoch 502/4000, AVG Loss:  3.647, AVG improvment: -24.025, Acc:  39.0%\n",
      "Epoch 503/4000, AVG Loss:  5.033, AVG improvment: -14.158, Acc:  31.0%\n",
      "Epoch 504/4000, AVG Loss:  5.360, AVG improvment: -19.002, Acc:  43.0%\n",
      "Epoch 505/4000, AVG Loss:  3.921, AVG improvment: -24.456, Acc:  47.0%\n",
      "Epoch 506/4000, AVG Loss:  3.584, AVG improvment: -17.009, Acc:  33.0%\n",
      "Epoch 507/4000, AVG Loss:  2.499, AVG improvment: -22.147, Acc:  45.0%\n",
      "Epoch 508/4000, AVG Loss:  3.870, AVG improvment: -21.104, Acc:  40.0%\n",
      "Epoch 509/4000, AVG Loss:  5.003, AVG improvment: -10.403, Acc:  34.0%\n",
      "Epoch 510/4000, AVG Loss:  2.828, AVG improvment: -18.479, Acc:  42.0%\n",
      "Epoch 511/4000, AVG Loss:  3.874, AVG improvment: -17.997, Acc:  44.0%\n",
      "Epoch 512/4000, AVG Loss:  3.181, AVG improvment: -11.086, Acc:  33.0%\n",
      "Epoch 513/4000, AVG Loss:  2.936, AVG improvment: -18.577, Acc:  43.0%\n",
      "Epoch 514/4000, AVG Loss:  2.704, AVG improvment: -23.279, Acc:  44.0%\n",
      "Epoch 515/4000, AVG Loss:  3.675, AVG improvment: -8.855, Acc:  32.0%\n",
      "Epoch 516/4000, AVG Loss:  4.696, AVG improvment: -13.662, Acc:  32.0%\n",
      "Epoch 517/4000, AVG Loss:  4.749, AVG improvment: -7.696, Acc:  31.0%\n",
      "Epoch 518/4000, AVG Loss:  3.204, AVG improvment: -17.971, Acc:  39.0%\n",
      "Epoch 519/4000, AVG Loss:  3.547, AVG improvment: -11.730, Acc:  36.0%\n",
      "Epoch 520/4000, AVG Loss:  2.937, AVG improvment: -22.203, Acc:  40.0%\n",
      "Epoch 521/4000, AVG Loss:  3.871, AVG improvment: -13.208, Acc:  42.0%\n",
      "Epoch 522/4000, AVG Loss:  4.684, AVG improvment: -18.516, Acc:  39.0%\n",
      "Epoch 523/4000, AVG Loss:  3.409, AVG improvment: -13.478, Acc:  36.0%\n",
      "Epoch 524/4000, AVG Loss:  5.109, AVG improvment: -11.592, Acc:  39.0%\n",
      "Epoch 525/4000, AVG Loss:  3.681, AVG improvment: -17.472, Acc:  38.0%\n",
      "Epoch 526/4000, AVG Loss:  2.480, AVG improvment: -9.980, Acc:  35.0%\n",
      "Epoch 527/4000, AVG Loss:  4.774, AVG improvment: -11.911, Acc:  38.0%\n",
      "Epoch 528/4000, AVG Loss:  2.954, AVG improvment: -14.938, Acc:  34.0%\n",
      "Epoch 529/4000, AVG Loss:  3.758, AVG improvment: -10.996, Acc:  30.0%\n",
      "Epoch 530/4000, AVG Loss:  3.911, AVG improvment: -11.827, Acc:  34.0%\n",
      "Epoch 531/4000, AVG Loss:  5.431, AVG improvment: -7.503, Acc:  32.0%\n",
      "Epoch 532/4000, AVG Loss:  4.757, AVG improvment: -12.700, Acc:  35.0%\n",
      "Epoch 533/4000, AVG Loss:  3.065, AVG improvment: -15.834, Acc:  39.0%\n",
      "Epoch 534/4000, AVG Loss:  4.045, AVG improvment: -19.987, Acc:  33.0%\n",
      "Epoch 535/4000, AVG Loss:  4.595, AVG improvment: -10.062, Acc:  32.0%\n",
      "Epoch 536/4000, AVG Loss:  4.934, AVG improvment: -12.977, Acc:  35.0%\n",
      "Epoch 537/4000, AVG Loss:  4.459, AVG improvment: -11.373, Acc:  37.0%\n",
      "Epoch 538/4000, AVG Loss:  3.097, AVG improvment: -14.341, Acc:  27.0%\n",
      "Epoch 539/4000, AVG Loss:  4.276, AVG improvment: -9.010, Acc:  29.0%\n",
      "Epoch 540/4000, AVG Loss:  3.457, AVG improvment: -11.967, Acc:  28.0%\n",
      "Epoch 541/4000, AVG Loss:  4.439, AVG improvment: -12.722, Acc:  31.0%\n",
      "Epoch 542/4000, AVG Loss:  3.825, AVG improvment: -14.209, Acc:  30.0%\n",
      "Epoch 543/4000, AVG Loss:  4.492, AVG improvment: -6.932, Acc:  30.0%\n",
      "Epoch 544/4000, AVG Loss:  4.254, AVG improvment: -5.783, Acc:  25.0%\n",
      "Epoch 545/4000, AVG Loss:  2.928, AVG improvment: -15.087, Acc:  38.0%\n",
      "Epoch 546/4000, AVG Loss:  2.949, AVG improvment: -9.723, Acc:  30.0%\n",
      "Epoch 547/4000, AVG Loss:  3.593, AVG improvment: -12.641, Acc:  28.0%\n",
      "Epoch 548/4000, AVG Loss:  4.267, AVG improvment: -13.361, Acc:  36.0%\n",
      "Epoch 549/4000, AVG Loss:  3.392, AVG improvment: -13.393, Acc:  32.0%\n",
      "Epoch 550/4000, AVG Loss:  3.511, AVG improvment: -15.248, Acc:  37.0%\n",
      "Epoch 551/4000, AVG Loss:  3.273, AVG improvment: -19.248, Acc:  36.0%\n",
      "Epoch 552/4000, AVG Loss:  4.352, AVG improvment: -8.659, Acc:  25.0%\n",
      "Epoch 553/4000, AVG Loss:  5.292, AVG improvment: -8.464, Acc:  29.0%\n",
      "Epoch 554/4000, AVG Loss:  4.059, AVG improvment: -8.822, Acc:  28.0%\n",
      "Epoch 555/4000, AVG Loss:  5.269, AVG improvment: -14.663, Acc:  37.0%\n",
      "Epoch 556/4000, AVG Loss:  4.344, AVG improvment: -7.160, Acc:  22.0%\n",
      "Epoch 557/4000, AVG Loss:  4.061, AVG improvment: -11.388, Acc:  33.0%\n",
      "Epoch 558/4000, AVG Loss:  4.033, AVG improvment: -10.507, Acc:  30.0%\n",
      "Epoch 559/4000, AVG Loss:  2.910, AVG improvment: -16.865, Acc:  38.0%\n",
      "Epoch 560/4000, AVG Loss:  4.281, AVG improvment: -17.710, Acc:  34.0%\n",
      "Epoch 561/4000, AVG Loss:  4.705, AVG improvment: -10.212, Acc:  35.0%\n",
      "Epoch 562/4000, AVG Loss:  5.616, AVG improvment: -14.070, Acc:  44.0%\n",
      "Epoch 563/4000, AVG Loss:  3.303, AVG improvment: -14.422, Acc:  38.0%\n",
      "Epoch 564/4000, AVG Loss:  3.097, AVG improvment: -12.010, Acc:  31.0%\n",
      "Epoch 565/4000, AVG Loss:  4.569, AVG improvment: -14.518, Acc:  35.0%\n",
      "Epoch 566/4000, AVG Loss:  3.598, AVG improvment: -9.734, Acc:  30.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 567/4000, AVG Loss:  3.055, AVG improvment: -15.502, Acc:  39.0%\n",
      "Epoch 568/4000, AVG Loss:  4.706, AVG improvment: -9.346, Acc:  31.0%\n",
      "Epoch 569/4000, AVG Loss:  4.776, AVG improvment: -12.202, Acc:  36.0%\n",
      "Epoch 570/4000, AVG Loss:  3.554, AVG improvment: -7.296, Acc:  27.0%\n",
      "Epoch 571/4000, AVG Loss:  4.379, AVG improvment: -6.032, Acc:  27.0%\n",
      "Epoch 572/4000, AVG Loss:  3.378, AVG improvment: -8.347, Acc:  34.0%\n",
      "Epoch 573/4000, AVG Loss:  3.641, AVG improvment: -9.752, Acc:  34.0%\n",
      "Epoch 574/4000, AVG Loss:  4.466, AVG improvment: -5.641, Acc:  25.0%\n",
      "Epoch 575/4000, AVG Loss:  4.937, AVG improvment: -8.798, Acc:  30.0%\n",
      "Epoch 576/4000, AVG Loss:  4.579, AVG improvment: -4.929, Acc:  21.0%\n",
      "Epoch 577/4000, AVG Loss:  4.012, AVG improvment: -8.847, Acc:  35.0%\n",
      "Epoch 578/4000, AVG Loss:  3.903, AVG improvment: -4.300, Acc:  25.0%\n",
      "Epoch 579/4000, AVG Loss:  5.127, AVG improvment: -6.374, Acc:  26.0%\n",
      "Epoch 580/4000, AVG Loss:  5.714, AVG improvment: -6.630, Acc:  28.0%\n",
      "Epoch 581/4000, AVG Loss:  4.384, AVG improvment: -6.450, Acc:  28.0%\n",
      "Epoch 582/4000, AVG Loss:  4.127, AVG improvment: -5.936, Acc:  21.0%\n",
      "Epoch 583/4000, AVG Loss:  4.048, AVG improvment: -8.758, Acc:  30.0%\n",
      "Epoch 584/4000, AVG Loss:  4.009, AVG improvment: -5.438, Acc:  19.0%\n",
      "Epoch 585/4000, AVG Loss:  5.658, AVG improvment: -5.450, Acc:  20.0%\n",
      "Epoch 586/4000, AVG Loss:  4.151, AVG improvment: -10.824, Acc:  33.0%\n",
      "Epoch 587/4000, AVG Loss:  4.443, AVG improvment: -10.720, Acc:  24.0%\n",
      "Epoch 588/4000, AVG Loss:  2.972, AVG improvment: -19.793, Acc:  29.0%\n",
      "Epoch 589/4000, AVG Loss:  4.337, AVG improvment: -10.613, Acc:  31.0%\n",
      "Epoch 590/4000, AVG Loss:  3.590, AVG improvment: -9.009, Acc:  21.0%\n",
      "Epoch 591/4000, AVG Loss:  5.520, AVG improvment: -10.818, Acc:  29.0%\n",
      "Epoch 592/4000, AVG Loss:  5.203, AVG improvment: -11.920, Acc:  28.0%\n",
      "Epoch 593/4000, AVG Loss:  4.018, AVG improvment: -12.125, Acc:  30.0%\n",
      "Epoch 594/4000, AVG Loss:  2.828, AVG improvment: -11.465, Acc:  24.0%\n",
      "Epoch 595/4000, AVG Loss:  5.800, AVG improvment: -9.102, Acc:  17.0%\n",
      "Epoch 596/4000, AVG Loss:  5.298, AVG improvment: -6.581, Acc:  23.0%\n",
      "Epoch 597/4000, AVG Loss:  4.796, AVG improvment: -7.307, Acc:  20.0%\n",
      "Epoch 598/4000, AVG Loss:  3.260, AVG improvment: -16.386, Acc:  28.0%\n",
      "Epoch 599/4000, AVG Loss:  4.245, AVG improvment: -14.396, Acc:  30.0%\n",
      "Epoch 600/4000, AVG Loss:  4.370, AVG improvment: -6.406, Acc:  27.0%\n",
      "Epoch 601/4000, AVG Loss:  3.548, AVG improvment: -8.368, Acc:  22.0%\n",
      "Modelsnapshot saved as:ModelRNNE600_L 3.55_I-8.37_P 0.22\n",
      "Epoch 602/4000, AVG Loss:  4.121, AVG improvment: -11.198, Acc:  31.0%\n",
      "Epoch 603/4000, AVG Loss:  3.438, AVG improvment: -11.156, Acc:  23.0%\n",
      "Epoch 604/4000, AVG Loss:  3.753, AVG improvment: -11.686, Acc:  30.0%\n",
      "Epoch 605/4000, AVG Loss:  4.183, AVG improvment: -8.466, Acc:  29.0%\n",
      "Epoch 606/4000, AVG Loss:  5.316, AVG improvment: -5.086, Acc:  27.0%\n",
      "Epoch 607/4000, AVG Loss:  5.206, AVG improvment: -5.729, Acc:  22.0%\n",
      "Epoch 608/4000, AVG Loss:  4.546, AVG improvment: -11.115, Acc:  27.0%\n",
      "Epoch 609/4000, AVG Loss:  3.808, AVG improvment: -5.238, Acc:  24.0%\n",
      "Epoch 610/4000, AVG Loss:  4.076, AVG improvment: -6.494, Acc:  22.0%\n",
      "Epoch 611/4000, AVG Loss:  3.787, AVG improvment: -10.925, Acc:  21.0%\n",
      "Epoch 612/4000, AVG Loss:  2.523, AVG improvment: -7.177, Acc:  19.0%\n",
      "Epoch 613/4000, AVG Loss:  3.723, AVG improvment: -10.537, Acc:  25.0%\n",
      "Epoch 614/4000, AVG Loss:  4.333, AVG improvment: -7.309, Acc:  20.0%\n",
      "Epoch 615/4000, AVG Loss:  3.539, AVG improvment: -8.750, Acc:  18.0%\n",
      "Epoch 616/4000, AVG Loss:  5.043, AVG improvment: -3.701, Acc:  17.0%\n",
      "Epoch 617/4000, AVG Loss:  3.896, AVG improvment: -8.037, Acc:  16.0%\n",
      "Epoch 618/4000, AVG Loss:  4.615, AVG improvment: -8.979, Acc:  18.0%\n",
      "Epoch 619/4000, AVG Loss:  3.936, AVG improvment: -7.387, Acc:  15.0%\n",
      "Epoch 620/4000, AVG Loss:  5.160, AVG improvment: -4.713, Acc:  13.0%\n",
      "Epoch 621/4000, AVG Loss:  4.765, AVG improvment: -9.169, Acc:  15.0%\n",
      "Epoch 622/4000, AVG Loss:  2.753, AVG improvment: -7.481, Acc:  11.0%\n",
      "Epoch 623/4000, AVG Loss:  3.942, AVG improvment: -8.365, Acc:  17.0%\n",
      "Epoch 624/4000, AVG Loss:  4.290, AVG improvment: -11.129, Acc:  19.0%\n",
      "Epoch 625/4000, AVG Loss:  5.211, AVG improvment: -10.279, Acc:  18.0%\n",
      "Epoch 626/4000, AVG Loss:  3.937, AVG improvment: -6.407, Acc:  14.0%\n",
      "Epoch 627/4000, AVG Loss:  4.567, AVG improvment: -4.466, Acc:  13.0%\n",
      "Epoch 628/4000, AVG Loss:  4.923, AVG improvment: -7.392, Acc:  14.0%\n",
      "Epoch 629/4000, AVG Loss:  3.976, AVG improvment: -4.799, Acc:  12.0%\n",
      "Epoch 630/4000, AVG Loss:  3.970, AVG improvment: -11.098, Acc:  20.0%\n",
      "Epoch 631/4000, AVG Loss:  3.236, AVG improvment: -11.926, Acc:  21.0%\n",
      "Epoch 632/4000, AVG Loss:  4.085, AVG improvment: -2.492, Acc:  12.0%\n",
      "Epoch 633/4000, AVG Loss:  3.989, AVG improvment: -5.692, Acc:  16.0%\n",
      "Epoch 634/4000, AVG Loss:  4.532, AVG improvment: -5.745, Acc:  19.0%\n",
      "Epoch 635/4000, AVG Loss:  4.264, AVG improvment: -7.232, Acc:  19.0%\n",
      "Epoch 636/4000, AVG Loss:  5.329, AVG improvment: -6.543, Acc:  13.0%\n",
      "Epoch 637/4000, AVG Loss:  3.998, AVG improvment: -8.668, Acc:  18.0%\n",
      "Epoch 638/4000, AVG Loss:  3.429, AVG improvment: -7.640, Acc:  13.0%\n",
      "Epoch 639/4000, AVG Loss:  4.563, AVG improvment: -3.890, Acc:  14.0%\n",
      "Epoch 640/4000, AVG Loss:  3.606, AVG improvment: -10.249, Acc:  21.0%\n",
      "Epoch 641/4000, AVG Loss:  3.906, AVG improvment: -6.348, Acc:  15.0%\n",
      "Epoch 642/4000, AVG Loss:  3.776, AVG improvment: -5.975, Acc:  11.0%\n",
      "Epoch 643/4000, AVG Loss:  3.963, AVG improvment: -8.484, Acc:  19.0%\n",
      "Epoch 644/4000, AVG Loss:  2.499, AVG improvment: -7.857, Acc:  18.0%\n",
      "Epoch 645/4000, AVG Loss:  4.699, AVG improvment: -6.399, Acc:  18.0%\n",
      "Epoch 646/4000, AVG Loss:  3.643, AVG improvment: -4.371, Acc:  17.0%\n",
      "Epoch 647/4000, AVG Loss:  3.818, AVG improvment: -5.010, Acc:  17.0%\n",
      "Epoch 648/4000, AVG Loss:  5.440, AVG improvment: -4.463, Acc:  16.0%\n",
      "Epoch 649/4000, AVG Loss:  3.678, AVG improvment: -9.884, Acc:  17.0%\n",
      "Epoch 650/4000, AVG Loss:  3.659, AVG improvment: -5.876, Acc:  12.0%\n",
      "Epoch 651/4000, AVG Loss:  4.082, AVG improvment: -4.072, Acc:  17.0%\n",
      "Epoch 652/4000, AVG Loss:  5.415, AVG improvment: -9.952, Acc:  13.0%\n",
      "Epoch 653/4000, AVG Loss:  4.207, AVG improvment: -6.436, Acc:  8.0%\n",
      "Epoch 654/4000, AVG Loss:  4.450, AVG improvment: -6.329, Acc:  10.0%\n",
      "Epoch 655/4000, AVG Loss:  4.922, AVG improvment: -3.642, Acc:  10.0%\n",
      "Epoch 656/4000, AVG Loss:  3.765, AVG improvment: -1.788, Acc:  3.0%\n",
      "Epoch 657/4000, AVG Loss:  3.887, AVG improvment: -5.817, Acc:  9.0%\n",
      "Epoch 658/4000, AVG Loss:  3.667, AVG improvment: -2.638, Acc:  6.0%\n",
      "Epoch 659/4000, AVG Loss:  3.365, AVG improvment: -2.332, Acc:  4.0%\n",
      "Epoch 660/4000, AVG Loss:  5.176, AVG improvment: -5.468, Acc:  8.0%\n",
      "Epoch 661/4000, AVG Loss:  4.010, AVG improvment: -0.597, Acc:  3.0%\n",
      "Epoch 662/4000, AVG Loss:  4.455, AVG improvment: -2.745, Acc:  6.0%\n",
      "Epoch 663/4000, AVG Loss:  4.499, AVG improvment: -6.036, Acc:  8.0%\n",
      "Epoch 664/4000, AVG Loss:  3.561, AVG improvment: -5.119, Acc:  10.0%\n",
      "Epoch 665/4000, AVG Loss:  5.116, AVG improvment: -1.893, Acc:  5.0%\n",
      "Epoch 666/4000, AVG Loss:  4.757, AVG improvment: -5.012, Acc:  8.0%\n",
      "Epoch 667/4000, AVG Loss:  4.019, AVG improvment: -3.455, Acc:  9.0%\n",
      "Epoch 668/4000, AVG Loss:  3.881, AVG improvment: -4.057, Acc:  10.0%\n",
      "Epoch 669/4000, AVG Loss:  4.799, AVG improvment: -2.326, Acc:  5.0%\n",
      "Epoch 670/4000, AVG Loss:  4.035, AVG improvment: -10.174, Acc:  13.0%\n",
      "Epoch 671/4000, AVG Loss:  3.898, AVG improvment: -1.511, Acc:  4.0%\n",
      "Epoch 672/4000, AVG Loss:  2.787, AVG improvment: -4.318, Acc:  10.0%\n",
      "Epoch 673/4000, AVG Loss:  5.069, AVG improvment: -7.691, Acc:  17.0%\n",
      "Epoch 674/4000, AVG Loss:  3.400, AVG improvment: -9.072, Acc:  20.0%\n",
      "Epoch 675/4000, AVG Loss:  4.024, AVG improvment: -5.415, Acc:  17.0%\n",
      "Epoch 676/4000, AVG Loss:  3.624, AVG improvment: -9.620, Acc:  16.0%\n",
      "Epoch 677/4000, AVG Loss:  4.558, AVG improvment: -4.608, Acc:  8.0%\n",
      "Epoch 678/4000, AVG Loss:  3.779, AVG improvment: -13.698, Acc:  23.0%\n",
      "Epoch 679/4000, AVG Loss:  3.306, AVG improvment: -7.476, Acc:  17.0%\n",
      "Epoch 680/4000, AVG Loss:  4.393, AVG improvment: -8.840, Acc:  21.0%\n",
      "Epoch 681/4000, AVG Loss:  4.005, AVG improvment: -9.593, Acc:  19.0%\n",
      "Epoch 682/4000, AVG Loss:  4.840, AVG improvment: -6.543, Acc:  20.0%\n",
      "Epoch 683/4000, AVG Loss:  3.522, AVG improvment: -6.776, Acc:  13.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 684/4000, AVG Loss:  3.483, AVG improvment: -6.447, Acc:  13.0%\n",
      "Epoch 685/4000, AVG Loss:  4.235, AVG improvment: -6.701, Acc:  14.0%\n",
      "Epoch 686/4000, AVG Loss:  3.902, AVG improvment: -4.289, Acc:  19.0%\n",
      "Epoch 687/4000, AVG Loss:  4.168, AVG improvment: -8.183, Acc:  28.0%\n",
      "Epoch 688/4000, AVG Loss:  3.818, AVG improvment: -7.697, Acc:  20.0%\n",
      "Epoch 689/4000, AVG Loss:  5.252, AVG improvment: -3.874, Acc:  13.0%\n",
      "Epoch 690/4000, AVG Loss:  4.899, AVG improvment: -6.049, Acc:  21.0%\n",
      "Epoch 691/4000, AVG Loss:  2.783, AVG improvment: -5.985, Acc:  16.0%\n",
      "Epoch 692/4000, AVG Loss:  3.754, AVG improvment: -11.900, Acc:  22.0%\n",
      "Epoch 693/4000, AVG Loss:  3.818, AVG improvment: -4.831, Acc:  13.0%\n",
      "Epoch 694/4000, AVG Loss:  4.088, AVG improvment: -5.053, Acc:  15.0%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9140/2141643018.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m                 \u001b[0mmodelIn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mallowedPVs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msectionPos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m                 \u001b[0mmodelOut\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodelIn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m                 \u001b[0maction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodelOut\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m                 \u001b[1;31m# We now have our next step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programme\\Anaconda\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programme\\Anaconda\\lib\\site-packages\\keras\\src\\engine\\training.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    588\u001b[0m             \u001b[0mlayout_map_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_subclass_model_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layout_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 590\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mdoc_controls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc_in_current_and_subclasses\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programme\\Anaconda\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programme\\Anaconda\\lib\\site-packages\\keras\\src\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1147\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1148\u001b[0m                 ):\n\u001b[1;32m-> 1149\u001b[1;33m                     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programme\\Anaconda\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[0mbound_signature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_keras_call_info_injected\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programme\\Anaconda\\lib\\site-packages\\keras\\src\\engine\\sequential.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    396\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 398\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m  \u001b[1;31m# handle the corner case where self.layers is empty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programme\\Anaconda\\lib\\site-packages\\keras\\src\\engine\\functional.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    513\u001b[0m             \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtensors\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mthere\u001b[0m \u001b[0mare\u001b[0m \u001b[0mmore\u001b[0m \u001b[0mthan\u001b[0m \u001b[0mone\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    514\u001b[0m         \"\"\"\n\u001b[1;32m--> 515\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_internal_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    516\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programme\\Anaconda\\lib\\site-packages\\keras\\src\\engine\\functional.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m                 \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_arguments\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 672\u001b[1;33m                 \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    673\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m                 \u001b[1;31m# Update tensor_dict.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programme\\Anaconda\\lib\\site-packages\\keras\\src\\layers\\rnn\\base_rnn.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 556\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    557\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    558\u001b[0m         \u001b[1;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programme\\Anaconda\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programme\\Anaconda\\lib\\site-packages\\keras\\src\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1147\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1148\u001b[0m                 ):\n\u001b[1;32m-> 1149\u001b[1;33m                     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programme\\Anaconda\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[0mbound_signature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_keras_call_info_injected\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programme\\Anaconda\\lib\\site-packages\\keras\\src\\layers\\rnn\\simple_rnn.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, mask, training, initial_state)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 411\u001b[1;33m         return super().call(\n\u001b[0m\u001b[0;32m    412\u001b[0m             \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m         )\n",
      "\u001b[1;32mD:\\Programme\\Anaconda\\lib\\site-packages\\keras\\src\\layers\\rnn\\base_rnn.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, mask, training, initial_state, constants)\u001b[0m\n\u001b[0;32m    720\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m         last_output, outputs, states = backend.rnn(\n\u001b[0m\u001b[0;32m    723\u001b[0m             \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    724\u001b[0m             \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programme\\Anaconda\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programme\\Anaconda\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1258\u001b[0m       \u001b[1;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1260\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1261\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1262\u001b[0m         \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programme\\Anaconda\\lib\\site-packages\\keras\\src\\backend.py\u001b[0m in \u001b[0;36mrnn\u001b[1;34m(step_function, inputs, initial_states, go_backwards, mask, constants, unroll, input_length, time_major, zero_output_for_mask, return_all_outputs)\u001b[0m\n\u001b[0;32m   5175\u001b[0m         \u001b[0moutput_ta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfinal_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5177\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moutput_ta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5178\u001b[0m         \u001b[0mlast_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programme\\Anaconda\\lib\\site-packages\\keras\\src\\backend.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   5175\u001b[0m         \u001b[0moutput_ta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfinal_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5177\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moutput_ta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5178\u001b[0m         \u001b[0mlast_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programme\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\tensor_array_ops.py\u001b[0m in \u001b[0;36mstack\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1214\u001b[0m       \u001b[0mAll\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtensors\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mTensorArray\u001b[0m \u001b[0mstacked\u001b[0m \u001b[0minto\u001b[0m \u001b[0mone\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m     \"\"\"\n\u001b[1;32m-> 1216\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_implementation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mgather\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programme\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\tensor_array_ops.py\u001b[0m in \u001b[0;36mstack\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    875\u001b[0m           np.ndarray([0] + self._element_shape), name=name, dtype=self._dtype)\n\u001b[0;32m    876\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 877\u001b[1;33m       return ops.convert_to_tensor(\n\u001b[0m\u001b[0;32m    878\u001b[0m           self._tensor_array, name=name, dtype=self._dtype)\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programme\\Anaconda\\lib\\site-packages\\tensorflow\\python\\profiler\\trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m           \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programme\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m    694\u001b[0m   \u001b[1;31m# TODO(b/142518781): Fix all call-sites and remove redundant arg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m   \u001b[0mpreferred_dtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreferred_dtype\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mdtype_hint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 696\u001b[1;33m   return tensor_conversion_registry.convert(\n\u001b[0m\u001b[0;32m    697\u001b[0m       \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreferred_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccepted_result_types\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m   )\n",
      "\u001b[1;32mD:\\Programme\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion_registry.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programme\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36m_autopacking_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m   1583\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0minferred_dtype\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1584\u001b[0m     \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_cast_nested_seqs_to_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1585\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0m_autopacking_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m\"packed\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programme\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36m_autopacking_helper\u001b[1;34m(list_or_tuple, dtype, name)\u001b[0m\n\u001b[0;32m   1490\u001b[0m     \u001b[1;31m# checking.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1491\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist_or_tuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1492\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_or_tuple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1493\u001b[0m   \u001b[0mmust_pack\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1494\u001b[0m   \u001b[0mconverted_elems\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programme\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mpack\u001b[1;34m(values, axis, name)\u001b[0m\n\u001b[0;32m   6713\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6714\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6715\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m   6716\u001b[0m         _ctx, \"Pack\", name, values, \"axis\", axis)\n\u001b[0;32m   6717\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "runsPerepisode = 100\n",
    "episodes = 4000\n",
    "\n",
    "bestLoss = 0\n",
    "bestDec = 0\n",
    "\n",
    "num_steps = 4.9\n",
    "print(\"Starting Traning...\")\n",
    "for episode in range(episodes):\n",
    "    \n",
    "    num_steps = min(num_steps + 0.01, 30)\n",
    "    sumList = [0,0,0]\n",
    "    for run in range(runsPerepisode):\n",
    "        # This is one run\n",
    "        \n",
    "        # Reset the LSTM states\n",
    "        model.layers[0].reset_states()\n",
    "        model.layers[1].reset_states() \n",
    "        model.layers[2].reset_states()\n",
    "        model.layers[3].reset_states()\n",
    "        \n",
    "        # Choose a starting position\n",
    "        position, sectionPos = get_StartingPossition()\n",
    "        \n",
    "        # Choose which PVs we can manipulate in this run\n",
    "        allowedPVs = generateRandomBoolArr(output_dim, 12)\n",
    "        \n",
    "        # Init loss\n",
    "        y = sim.get(position)\n",
    "        minY = y\n",
    "        startY = y\n",
    "        \n",
    "        states = []\n",
    "        returns = []\n",
    "        action = []\n",
    "        fail = 0\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass\n",
    "            for step in range(math.floor(num_steps)):\n",
    "\n",
    "                modelIn = np.reshape(np.concatenate((allowedPVs, sectionPos, [y]), axis=0), (1, 1, input_dim))\n",
    "                modelOut = model(modelIn)\n",
    "                action.append(modelOut)\n",
    "                # We now have our next step\n",
    "                position , sectionPos = modelOutToPosition(modelOut.numpy()[0], allowedPVs, position)\n",
    "                # We can calculate the loss right here\n",
    "                lastY = y\n",
    "                y = sim.get(position)\n",
    "                returns.append((min(y-minY, 0) + (y-lastY))/ math.floor(num_steps))\n",
    "                \n",
    "                \n",
    "            \n",
    "                if y < minY: \n",
    "                    minY = y\n",
    "            #We made a run Now to calculate a gradient\n",
    "            result = [(action * allowedPVs/ tf.reduce_sum(abs(action * allowedPVs))) * returns for action, returns in zip(action, returns)]\n",
    "            loss = tf.reduce_sum(result) \n",
    "\n",
    "        # Backward pass\n",
    "        gradients = tape.gradient(tf.constant(loss), model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))   \n",
    "        sumList[0] += loss\n",
    "        sumList[1] += minY - startY\n",
    "        if startY != minY:\n",
    "            sumList[2] += 1\n",
    "\n",
    "    # Print episode information\n",
    "    print(f'episode {episode+1}/{episodes}, AVG Loss: {sumList[0]/runsPerepisode: .3f}, AVG improvment: {sumList[1]/runsPerepisode: .3f}, Acc: {(sumList[2]/runsPerepisode)*100: .1f}%')\n",
    "    \n",
    "    if sumList[0]/runsPerepisode < bestLoss:\n",
    "        bestLoss = sumList[0]/runsPerepisode\n",
    "        print(f'------------------------ New best Loss with {bestLoss: .3f} ------------------------')\n",
    "        model.save('BestLossRNN.keras')\n",
    "    \n",
    "    if (sumList[1]/runsPerepisode) * (sumList[2]/runsPerepisode) < bestDec:\n",
    "        bestDec = (sumList[1]/runsPerepisode) * (sumList[2]/runsPerepisode)\n",
    "        print(f'------------------------ New best Dec with {bestDec: .3f} ------------------------')\n",
    "        model.save('BestDecRNN.keras')\n",
    "    \n",
    "    if episode % 100 == 0:\n",
    "        name = f'ModelRNNE{episode}_L{sumList[0]/runsPerepisode: .2f}_I{sumList[1]/runsPerepisode: .2f}_P{sumList[2]/runsPerepisode: .2f}'\n",
    "        print(\"Modelsnapshot saved as:\" + name)\n",
    "        model.save(name + '.keras')\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
